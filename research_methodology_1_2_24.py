# -*- coding: utf-8 -*-
"""Research methodology 1/2/24

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYlUAdvkYupoZTHxggs1CeMAOMWJFFP1
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


file_path='/content/drive/MyDrive/diabetes_data.csv'
df = pd.read_csv(file_path)
print(df.head())

df.Diabetes.unique()

df.head(20)

df.columns

df.info()

print(df.shape[0])
print(df.shape[1])

df.nunique()

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.duplicated().sum()

df.dtypes

print(df['Diabetes'].value_counts())

print(df.isnull().sum())
print(df.describe())

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

df.hist(figsize=(12, 8), bins=20, edgecolor="black")
plt.suptitle("Feature Distributions", fontsize=14)
plt.show()

all_values = df.melt().value_counts()

# Define labels and sizes
# Convert the MultiIndex to strings by accessing individual levels and joining them
labels = all_values.index.map(lambda x: ', '.join(map(str, x)))[:10]  # Show top 10 most frequent values
sizes = all_values.values[:10]  # Corresponding frequencies

# Define colors
colors = plt.cm.Paired.colors[:10]

# Plot a single pie chart
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=140, colors=colors, shadow=True)
plt.title("Distribution of Values Across All Columns")
plt.show()

# Define numerical columns for histogram
numerical_features = ['Age', 'BMI', 'MentHlth', 'PhysHlth']

# Plot histograms
df[numerical_features].hist(figsize=(12, 8), bins=30, color='skyblue', edgecolor='black')

plt.suptitle("Histograms of Numerical Features", fontsize=16)
plt.show()

# Define categorical columns for pie charts
categorical_features = ['Diabetes', 'Sex', 'Smoker']

# Create pie charts
fig, axes = plt.subplots(1, len(categorical_features), figsize=(15, 5))

for i, col in enumerate(categorical_features):
    df[col].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[i], colors=['lightblue', 'lightcoral'])
    axes[i].set_title(f"Distribution of {col}")
    axes[i].set_ylabel('')

plt.suptitle("Pie Charts of Categorical Features", fontsize=16)
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(x='BMI', y='Diabetes', data=df, hue='Diabetes', palette='coolwarm')
plt.title('BMI vs Diabetes')
plt.xlabel('BMI')
plt.ylabel('Diabetes (0: No, 1: Yes)')
plt.show()

# Create scatter plot for Age vs BMI without title
plt.figure(figsize=(6, 4))
plt.scatter(df["Age"], df["BMI"], alpha=0.5, color="purple")
plt.xlabel("Age")
plt.ylabel("BMI")
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df["MentHlth"], df["BMI"], alpha=0.5, color="purple")
plt.xlabel("Mental Health (MentHlth)")
plt.ylabel("BMI")
plt.grid(True, linestyle="--", alpha=0.5)
plt.title("MentHlth vs BMI")
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df["PhysHlth"], df["BMI"], alpha=0.5, color="blue")
plt.xlabel("Physical Health (PhysHlth)")
plt.ylabel("BMI")
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()

# Select only numerical columns for the pairplot
numerical_features = ['Age', 'BMI', 'MentHlth', 'PhysHlth']
df_numeric = df[numerical_features]

# Set seaborn style for better visuals
sns.set(style="whitegrid")

# Create a pair plot without specifying 'hue' or 'palette' for better alignment
g = sns.pairplot(df_numeric, diag_kind='hist', plot_kws={'alpha': 0.6})

# Display the plot
plt.show()

plt.figure(figsize=(12, 6))
df.boxplot()
plt.xticks(rotation=45)
plt.title("Boxplot of Features (Detect Outliers)")
plt.show()

# Calculate z-scores for numerical columns
z_scores = df.apply(zscore)

# Set a threshold (e.g., 3)
threshold = 3

# Find outliers
outliers_z = {}
for column in df:
    outliers_z[column] = df[z_scores[column].abs() > threshold]
    print(f"Outliers in column '{column}': {len(outliers_z[column])} rows")

def handle_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[column] = np.clip(data[column], lower_bound, upper_bound)
    return data

for feature in df:
    df = handle_outliers_iqr(df, feature)

# Calculate z-scores for numerical columns
z_scores = df.apply(zscore)

# Set a threshold (e.g., 3)
threshold = 3

# Find outliers
outliers_z = {}
for column in df:
    outliers_z[column] = df[z_scores[column].abs() > threshold]
    print(f"Outliers in column '{column}': {len(outliers_z[column])} rows")

plt.figure(figsize=(12, 6))
df.boxplot()
plt.xticks(rotation=45)
plt.title("Boxplot of Features (Detect Outliers)")
plt.show()

X = df.drop(columns=['Diabetes'])  # Assuming 'Diabetes' is the target column
y = df['Diabetes']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Train Logistic Regression Model
model = LogisticRegression(max_iter=5000, solver='saga')
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

# Train Decision Tree Model
model = DecisionTreeClassifier(random_state=42, max_depth=5)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix - Decision Tree")
plt.show()

# Train Random Forest Model
model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix - Random Forest")
plt.show()

# Train SVM Model
model = SVC(kernel='rbf', C=1.0, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix - SVM")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
# Train k-NN Model
k = 5  # Choose the number of neighbors
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title(f"Confusion Matrix - k-NN (k={k})")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/diabetes_data_cleaned.csv")

# Define features and target
X = df.drop(columns=["Diabetes"])
y = df["Diabetes"]

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train SVM model
svm_model = SVC(kernel="rbf", C=1.0, gamma="scale", random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = svm_model.predict(X_test_scaled)

# Output predictions
print("Predicted Outputs:")
print(y_pred)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
import numpy as np

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/diabetes_data_cleaned.csv")

# Encode categorical variables
label_encoders = {}
categorical_features = ["Sex"]  # Add more categorical features if necessary
for feature in categorical_features:
    le = LabelEncoder()
    df[feature] = le.fit_transform(df[feature].astype(str).str.lower())  # Ensure lowercase strings
    label_encoders[feature] = le

# Define features and target
X = df.drop(columns=["Diabetes"])
y = df["Diabetes"]

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train SVM model
svm_model = SVC(kernel="rbf", C=1.0, gamma="scale", random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = svm_model.predict(X_test_scaled)

# Output predictions
print("Predicted Outputs:")
print(y_pred)

# Function to predict from user input
def predict_user_input():
    print("Enter the following values:")
    user_input = []
    feature_names = ["Age", "Sex", "HighChol", "CholCheck", "BMI", "Smoker", "HeartDiseaseorAttack",
                     "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "GenHlth", "MentHlth",
                     "PhysHlth", "DiffWalk", "Stroke", "HighBP"]

    for feature in feature_names:
        if feature in categorical_features:
            while True:
                value = input(f"{feature} (e.g., male/female): ").strip().lower()
                if value in label_encoders[feature].classes_:
                    value = label_encoders[feature].transform([value])[0]
                    break
                else:
                    print(f"Invalid input! Please enter one of: {list(label_encoders[feature].classes_)}")
        else:
            value = float(input(f"{feature}: "))
        user_input.append(value)

    user_input_scaled = scaler.transform([user_input])
    prediction = svm_model.predict(user_input_scaled)
    print("Predicted class for user input:", prediction[0])

# Call the function to get user input and predict
predict_user_input()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
import numpy as np

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/diabetes_data_cleaned.csv")

# Select important features based on domain knowledge
important_features = ["Age", "Sex", "BMI", "HighChol", "Smoker", "HeartDiseaseorAttack", "PhysActivity", "HighBP"]

# Encode categorical variables
label_encoders = {}
categorical_features = ["Sex", "Smoker", "HeartDiseaseorAttack", "PhysActivity", "HighChol", "HighBP"]
for feature in categorical_features:
    le = LabelEncoder()
    df[feature] = le.fit_transform(df[feature].astype(str).str.lower())  # Ensure lowercase strings
    label_encoders[feature] = le

# Define features and target
X = df[important_features]
y = df["Diabetes"]

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train SVM model
svm_model = SVC(kernel="rbf", C=1.0, gamma="scale", random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = svm_model.predict(X_test_scaled)

# Output predictions
print("Predicted Outputs:")
print(y_pred)

# Function to predict from user input
def predict_user_input():
    print("Enter the following values:")
    user_input = []
    feature_examples = {
        "Age": "(e.g., 45)",
        "Sex": "(male/female)",
        "BMI": "(e.g., 25.4 - Body Mass Index)",
        "HighChol": "(yes/no - High Cholesterol)",
        "Smoker": "(yes/no)",
        "HeartDiseaseorAttack": "(yes/no)",
        "PhysActivity": "(yes/no - Physical Activity)",
        "HighBP": "(yes/no - High Blood Pressure)"
    }

    for feature in important_features:
        if feature in categorical_features:
            while True:
                value = input(f"{feature} {feature_examples[feature]}: ").strip().lower()
                if value in label_encoders[feature].classes_:
                    value = label_encoders[feature].transform([value])[0]
                    break
                else:
                    print(f"Invalid input! Please enter one of: {list(label_encoders[feature].classes_)}")
        else:
            value = float(input(f"{feature} {feature_examples[feature]}: "))
        user_input.append(value)

    user_input_scaled = scaler.transform([user_input])
    prediction = svm_model.predict(user_input_scaled)
    result = "Diabetic Person" if prediction[0] == 1 else "Non-Diabetic Person"
    print("Predicted class for user input:", result)

# Call the function to get user input and predict
predict_user_input()